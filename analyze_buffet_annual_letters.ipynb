{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_txt(directory: str) -> dict:\n",
    "    \"\"\"Read in all TXT files in the folder and store the text into memory\n",
    "    as a dictionary with special characters removed and converted to lowercase\n",
    "\n",
    "    Args:\n",
    "        directory (str): Folder name where all letter files are stored\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with year as the key and the cleaned\n",
    "            (lowercase and special characters removed) text as the value\n",
    "    \"\"\"\n",
    "\n",
    "    letters = {}\n",
    "\n",
    "    # Ensure that the directory exists\n",
    "    assert os.path.isdir(directory)\n",
    "\n",
    "    # Walk through the directory with the letter files\n",
    "    for curr_path, directories, files in os.walk(directory):\n",
    "\n",
    "        # Extract year values to use as the key\n",
    "        year = curr_path[-4:]\n",
    "\n",
    "        # Find all TXT files\n",
    "        for file in files:\n",
    "            if file.endswith('txt'):\n",
    "\n",
    "                # Read in the raw text of each TXT file\n",
    "                with open(os.path.join(os.getcwd(), directory, year, file),\n",
    "                          'r', encoding='ISO-8859-1') as f:\n",
    "                    raw_text = f.read()\n",
    "\n",
    "                # Convert to lower-case and remove special characters\n",
    "                clean_text = re.sub(r'[^\\w\\d&]', ' ', raw_text.lower())\n",
    "                letters[year] = clean_text\n",
    "\n",
    "    return letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = collect_txt('letters')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import NLTK library and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jlee0\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process text to prepare for text analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(letters: dict) -> dict:\n",
    "    \"\"\"Tokenize annual letters using NLTK's tokenizer\n",
    "\n",
    "    Args:\n",
    "        letters (dict): Dictionary with year as the key and the cleaned\n",
    "            (lowercase and special characters removed) text as the value\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with year as the key and a list of tokenized text as the value\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized = {}\n",
    "\n",
    "    # Tokenize each document by splitting the document into tokens\n",
    "    for year, text in letters.items():\n",
    "        tokenized[year] = word_tokenize(text)\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers\n",
    "    tokenized = {year: [token for token in text if not token.isnumeric()] for year, text in tokenized.items()}\n",
    "\n",
    "    # Remove words that are only one character\n",
    "    tokenized = {year: [token for token in text if len(token) > 1] for year, text in tokenized.items()}\n",
    "\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenize_text(letters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized: dict) -> dict:\n",
    "    \"\"\"Remove English stopwords from annual letters\n",
    "\n",
    "    Args:\n",
    "        tokenized (dict): Dictionary with year as the key and the tokenized text as the value\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with year as the key and a list of stop-word removed tokens as the value\n",
    "    \"\"\"\n",
    "\n",
    "    no_stopwords = {}\n",
    "\n",
    "    for year, text in tqdm(tokenized.items()):\n",
    "        no_stopwords[year] = [token for token in text if token not in stopwords.words('english')]\n",
    "\n",
    "    return no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [05:15<00:00,  6.85s/it]\n"
     ]
    }
   ],
   "source": [
    "no_stopwords = remove_stopwords(tokenized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(no_stopwords: dict) -> dict:\n",
    "    \"\"\"Lemmatize each stopword-removed list of tokens to standardize the tokens\n",
    "    for each letter\n",
    "\n",
    "    Args:\n",
    "        no_stopwords (dict): Dictionary with year as the key and a list of stop-word removed tokens as the value\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with year as the key and a list of lemmatized and \n",
    "            stop-word removed tokens as the value\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized = {}\n",
    "    for year, text in tqdm(no_stopwords.items()):\n",
    "        lemmatized[year] = [lemmatizer.lemmatize(token) for token in text]\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:02<00:00, 20.36it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmatized = lemmatize_text(no_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
